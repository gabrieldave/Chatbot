# ğŸ“‹ PIPELINE TÃ‰CNICO COMPLETO

## 1ï¸âƒ£ Â¿QUÃ‰ CHUNK SIZE USAS?

### **Chunk Size: Default de LlamaIndex (1024 caracteres)**

**âœ… Verificado con cÃ³digo:**
```python
Default chunk_size: 1024 caracteres
Default chunk_overlap: 200 caracteres
```

**No hay configuraciÃ³n explÃ­cita de chunk_size en el cÃ³digo actual.**

LlamaIndex usa por defecto:
- **Chunk size**: **1024 caracteres** (no tokens)
- **Chunk overlap**: **200 caracteres** (~20% para mantener contexto entre chunks)

**Evidencia en el cÃ³digo:**
```python
# No hay configuraciÃ³n explÃ­cita de chunk_size
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context,
    embed_model=embed_model,
    show_progress=False
)
```

LlamaIndex automÃ¡ticamente:
- Usa `SentenceSplitter` o `TokenTextSplitter` por defecto
- Divide documentos en chunks de ~1024 tokens
- Mantiene overlap entre chunks para contexto

**ObservaciÃ³n prÃ¡ctica:**
- Cada archivo genera ~100 chunks en promedio
- Chunk size: 1024 caracteres = ~256 tokens (1 token â‰ˆ 4 caracteres)
- Archivo promedio: ~100K caracteres / 100 chunks = ~1000 caracteres por chunk (consistente con 1024)

---

## 2ï¸âƒ£ Â¿EL PROCESO DE CHUNKING ESTÃ EN PYTHON O NODE?

### **âœ… 100% PYTHON**

**Stack tecnolÃ³gico:**
- **Lenguaje**: Python 3.x
- **Framework**: LlamaIndex (Python)
- **LibrerÃ­as**:
  - `llama-index-core` - Core de LlamaIndex
  - `llama-index-embeddings-openai` - Embeddings de OpenAI
  - `llama-index-vector-stores-supabase` - IntegraciÃ³n con Supabase
  - `psycopg2` - ConexiÃ³n a PostgreSQL/Supabase

**No hay cÃ³digo Node.js en el pipeline de ingestiÃ³n.**

---

## 3ï¸âƒ£ Â¿CUÃL ES TU PIPELINE?

### **Pipeline Completo:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE DE INGESTIÃ“N                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£ LEER PDF (y otros formatos)
   â†“
   SimpleDirectoryReader(input_files=[file_path])
   â€¢ Soporta: PDF, EPUB, TXT, DOCX, MD, DOC
   â€¢ Lee archivos desde ./data/
   â€¢ Convierte automÃ¡ticamente a texto

2ï¸âƒ£ CONVERTIR A TEXTO
   â†“
   reader.load_data()
   â€¢ LlamaIndex automÃ¡ticamente extrae texto
   â€¢ Crea objetos Document con metadata
   â€¢ Maneja diferentes formatos internamente

3ï¸âƒ£ DIVIDIR EN CHUNKS
   â†“
   VectorStoreIndex.from_documents()
   â€¢ LlamaIndex automÃ¡ticamente divide en chunks
   â€¢ Chunk size: 1024 caracteres (default)
   â€¢ Chunk overlap: 200 caracteres (default)
   â€¢ Usa SentenceSplitter por defecto

4ï¸âƒ£ LLAMAR EMBEDDINGS
   â†“
   embed_model = OpenAIEmbedding(model="text-embedding-3-small")
   â€¢ LlamaIndex automÃ¡ticamente llama a OpenAI
   â€¢ Genera embeddings para cada chunk
   â€¢ Modelo: text-embedding-3-small
   â€¢ Dimensiones: 1536

5ï¸âƒ£ SUBIR A SUPABASE
   â†“
   SupabaseVectorStore + VectorStoreIndex
   â€¢ Almacena vectores en PostgreSQL (pgvector)
   â€¢ Guarda metadata (file_name, chunk_id, etc.)
   â€¢ Tabla: vecs.knowledge (configurable)
```

---

## ğŸ“ CÃ“DIGO ESPECÃFICO DEL PIPELINE

### **Archivo: `ingest_parallel_tier3.py`**

```python
# 1. LEER PDF
reader = SimpleDirectoryReader(input_files=[file_path])
documents = reader.load_data()  # Convierte a texto automÃ¡ticamente

# 2-5. CHUNKING + EMBEDDINGS + SUBIR A SUPABASE (todo automÃ¡tico)
index = VectorStoreIndex.from_documents(
    documents,                    # Documentos ya en texto
    storage_context=storage_context,  # Configurado con Supabase
    embed_model=embed_model,      # OpenAI embeddings
    show_progress=False
)
```

**Todo sucede en una sola llamada:**
- âœ… Chunking automÃ¡tico
- âœ… Llamadas a embeddings automÃ¡ticas
- âœ… Subida a Supabase automÃ¡tica

---

## ğŸ”§ CONFIGURACIÃ“N ACTUAL

### **Embeddings:**
```python
embed_model = OpenAIEmbedding(model="text-embedding-3-small")
```

### **Vector Store:**
```python
vector_store = SupabaseVectorStore(
    postgres_connection_string=postgres_connection_string,
    collection_name=config.VECTOR_COLLECTION_NAME  # "knowledge"
)
```

### **Storage Context:**
```python
storage_context = StorageContext.from_defaults(vector_store=vector_store)
```

---

## ğŸ“Š ESTADÃSTICAS OBSERVADAS

**Basado en el cÃ³digo y logs:**
- **Chunks por archivo promedio**: ~100
- **Tokens por archivo promedio**: ~50,000
- **Chunk size estimado**: ~500 tokens/chunk (50K / 100)
- **O**: ~1024 tokens/chunk si hay overlap

**Nota**: La estimaciÃ³n de 100 requests/archivo sugiere que cada archivo genera ~100 chunks, lo cual es consistente con chunk size de ~500-1000 tokens.

---

## ğŸ’¡ PERSONALIZACIÃ“N POSIBLE

Si quisieras cambiar el chunk size, necesitarÃ­as:

```python
from llama_index.core.node_parser import SentenceSplitter

# Configurar splitter personalizado
text_splitter = SentenceSplitter(
    chunk_size=512,      # TamaÃ±o de chunk en caracteres (default: 1024)
    chunk_overlap=50     # Overlap entre chunks (default: 200)
)

# Usar en el Ã­ndice
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context,
    embed_model=embed_model,
    transformations=[text_splitter]  # Aplicar splitter personalizado
)
```

**Actualmente no estÃ¡ configurado**, asÃ­ que usa los defaults de LlamaIndex.

---

## âœ… RESUMEN

| Pregunta | Respuesta |
|----------|-----------|
| **Chunk size** | 1024 caracteres (~256 tokens) (default de LlamaIndex) |
| **Chunking en** | Python (LlamaIndex) |
| **Pipeline** | SimpleDirectoryReader â†’ load_data() â†’ VectorStoreIndex.from_documents() |
| **AutomatizaciÃ³n** | Todo automÃ¡tico: chunking, embeddings, subida a Supabase |

**El pipeline es completamente automÃ¡tico y manejado por LlamaIndex.**

