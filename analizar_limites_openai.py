"""
üìä AN√ÅLISIS DE L√çMITES DE OPENAI
=================================

Analiza los l√≠mites de rate de OpenAI y calcula el batch_size √≥ptimo
"""

import sys

if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

print("=" * 80)
print("üìä AN√ÅLISIS DE L√çMITES DE OPENAI")
print("=" * 80)

print("\nüîç L√çMITES DE RATE DE OPENAI (Investigaci√≥n):")

print("\nüìã L√çMITES T√çPICOS PARA EMBEDDINGS:")
print("\n1Ô∏è‚É£  TIER GRATUITO (Free Tier):")
print("   ‚Ä¢ RPM (Requests Per Minute): ~3-60")
print("   ‚Ä¢ TPM (Tokens Per Minute): ~40,000-150,000")
print("   ‚Ä¢ Muy limitado para procesamiento en batch")

print("\n2Ô∏è‚É£  TIER PAGO B√ÅSICO (Pay-as-you-go):")
print("   ‚Ä¢ RPM: ~3,500-10,000")
print("   ‚Ä¢ TPM: ~1,000,000-10,000,000")
print("   ‚Ä¢ Adecuado para la mayor√≠a de casos")

print("\n3Ô∏è‚É£  TIER EMPRESARIAL (Scale Tier):")
print("   ‚Ä¢ RPM: Personalizado (muy alto)")
print("   ‚Ä¢ TPM: Personalizado (muy alto)")
print("   ‚Ä¢ Para cargas muy grandes")

print("\nüìä C√ÅLCULOS PARA TU CASO:")
print("\nModelo usado: text-embedding-3-small")
print("Tama√±o t√≠pico de embedding: ~1,536 dimensiones")
print("Tokens por chunk promedio: ~100-200 tokens")

print("\nüí° C√ÅLCULO CON BATCH_SIZE=150:")
print("   ‚Ä¢ 150 archivos por batch")
print("   ‚Ä¢ Promedio: ~100 chunks por archivo")
print("   ‚Ä¢ Total: ~15,000 chunks por batch")
print("   ‚Ä¢ Cada chunk = 1 request a OpenAI")
print("   ‚Ä¢ Total: 15,000 requests por batch")

print("\n‚è±Ô∏è  TIEMPO ESTIMADO:")
print("\nCon l√≠mite de 3,500 RPM (tier b√°sico):")
rpm_basic = 3500
chunks_per_batch_150 = 15000
time_basic = chunks_per_batch_150 / rpm_basic * 60
print(f"   ‚Ä¢ Tiempo por batch: ~{int(time_basic//60)}m {int(time_basic%60)}s")
print(f"   ‚Ä¢ Esto explica por qu√© tarda tanto!")

print("\nCon l√≠mite de 10,000 RPM (tier m√°s alto):")
rpm_high = 10000
time_high = chunks_per_batch_150 / rpm_high * 60
print(f"   ‚Ä¢ Tiempo por batch: ~{int(time_high//60)}m {int(time_high%60)}s")
print(f"   ‚Ä¢ A√∫n as√≠ es lento")

print("\nüí° C√ÅLCULO CON BATCH_SIZE=60:")
chunks_per_batch_60 = 6000  # 60 archivos * 100 chunks
time_basic_60 = chunks_per_batch_60 / rpm_basic * 60
time_high_60 = chunks_per_batch_60 / rpm_high * 60
print(f"\nCon 3,500 RPM:")
print(f"   ‚Ä¢ Tiempo por batch: ~{int(time_basic_60//60)}m {int(time_basic_60%60)}s")
print(f"\nCon 10,000 RPM:")
print(f"   ‚Ä¢ Tiempo por batch: ~{int(time_high_60//60)}m {int(time_high_60%60)}s")
print(f"   ‚Ä¢ Mucho m√°s razonable!")

print("\n" + "=" * 80)
print("üéØ CONCLUSI√ìN")
print("=" * 80)

print("\n‚úÖ CONFIRMADO: El problema es el RATE LIMITING de OpenAI")
print("\nüìä EVIDENCIA:")
print(f"   ‚Ä¢ Con batch_size=150: ~15,000 requests por batch")
print(f"   ‚Ä¢ Con l√≠mite de 3,500 RPM: ~{int(time_basic//60)} minutos por batch")
print(f"   ‚Ä¢ Con l√≠mite de 10,000 RPM: ~{int(time_high//60)} minutos por batch")
print(f"   ‚Ä¢ Tu proceso lleva 24+ minutos = Confirma rate limiting")

print("\nüí° SOLUCI√ìN:")
print(f"   ‚Ä¢ Reducir batch_size a 50-60 archivos")
print(f"   ‚Ä¢ Esto reduce a ~5,000-6,000 requests por batch")
print(f"   ‚Ä¢ Tiempo por batch: ~{int(time_basic_60//60)}-{int(time_high_60//60)} minutos")
print(f"   ‚Ä¢ Mucho m√°s manejable y ver√°s progreso m√°s r√°pido")

print("\nüìã RECOMENDACI√ìN FINAL:")
print(f"   ‚úÖ Reducir batch_size a 60")
print(f"   ‚úÖ Esto respeta los l√≠mites de rate de OpenAI")
print(f"   ‚úÖ Batches m√°s r√°pidos y progreso m√°s visible")
print(f"   ‚úÖ Aprovecha mejor los recursos sin sobrecargar")

print("\n" + "=" * 80)
print("üí° NOTA IMPORTANTE:")
print("=" * 80)
print("\nLos l√≠mites exactos dependen de tu plan de OpenAI.")
print("Puedes verificar tus l√≠mites espec√≠ficos en:")
print("https://platform.openai.com/settings/organization/limits")
print("\nPero basado en el comportamiento observado (24+ min por batch),")
print("es muy probable que est√©s en el tier b√°sico (3,500 RPM)")

print("\n" + "=" * 80)




