# üèóÔ∏è REGLAS DE ARQUITECTURA RAG - SISTEMA DE INGESTA

## üìã CONTEXTO DEL PROYECTO

Este proyecto implementa un sistema RAG (Retrieval-Augmented Generation) para procesar y consultar documentos (principalmente libros en PDF) usando:

- **Framework**: LlamaIndex (Python)
- **Embeddings**: OpenAI `text-embedding-3-small` (1536 dimensiones)
- **Vector Store**: Supabase con pgvector (PostgreSQL)
- **Modelo de embeddings**: `text-embedding-3-small`
- **Tier OpenAI**: Tier 3 con l√≠mites:
  - 5,000 RPM (requests por minuto)
  - 5,000,000 TPM (tokens por minuto)
  - 100,000,000 TPD (tokens por d√≠a)

## üéØ OBJETIVO DE RENDIMIENTO

**Usar aproximadamente el 70% de la capacidad de Tier 3** para mantener estabilidad:
- **RPM objetivo**: 3,500 (70% de 5,000)
- **TPM objetivo**: 3,500,000 (70% de 5,000,000)
- **NO exceder estos l√≠mites** salvo solicitud expl√≠cita del usuario

## ‚öôÔ∏è CONFIGURACI√ìN POR DEFECTO (OBLIGATORIA)

### Chunking
- **Chunk size**: **1024 caracteres** (NO tokens, caracteres)
- **Chunk overlap**: **200 caracteres**
- **Splitter**: `SentenceSplitter` de LlamaIndex
- **Excepci√≥n**: Solo cambiar si el usuario lo solicita expl√≠citamente

### Embeddings
- **Modelo**: `text-embedding-3-small` (1536 dimensiones)
- **Batch size para embeddings**: **30-40 chunks por request** (por defecto: 30)
- **Excepci√≥n**: Solo cambiar si el usuario lo solicita expl√≠citamente

### Workers y Concurrencia
- **N√∫mero de workers por defecto**: **15**
- **Debe ser configurable** mediante variable de entorno `MAX_WORKERS`
- **Soporte para procesamiento paralelo** usando `asyncio` o `concurrent.futures`

### Base de Datos
- **Vector Store**: Supabase con pgvector
- **Dimensiones**: 1536 (compatible con `text-embedding-3-small`)
- **Metadatos obligatorios a guardar**:
  - `file_name`: Nombre del archivo
  - `chunk_id`: ID/n√∫mero del chunk
  - `chunk_index`: √çndice del chunk en el documento
  - `char_range`: Rango de caracteres (start, end)
  - `page_range`: Rango de p√°ginas (si est√° disponible)
  - `book_title`: T√≠tulo del libro/documento
  - `total_chunks`: Total de chunks del documento

## üîí REGLAS DE C√ìDIGO

### 1. Chunking
```python
# SIEMPRE usar esta configuraci√≥n por defecto:
from llama_index.core.node_parser import SentenceSplitter

text_splitter = SentenceSplitter(
    chunk_size=1024,      # 1024 caracteres (NO tokens)
    chunk_overlap=200    # 200 caracteres de overlap
)
```

**NO cambiar estos valores** salvo solicitud expl√≠cita del usuario.

### 2. Embeddings
```python
# SIEMPRE usar este modelo por defecto:
from llama_index.embeddings.openai import OpenAIEmbedding

embed_model = OpenAIEmbedding(model="text-embedding-3-small")
```

**NO cambiar el modelo** salvo solicitud expl√≠cita del usuario.

### 3. Batch Size para Embeddings
- **Por defecto**: 30 chunks por request a OpenAI
- **Rango permitido**: 30-40 chunks
- **Configurable mediante**: Variable de entorno `EMBEDDING_BATCH_SIZE`

### 4. Rate Limiting y Control de Carga
- **Implementar sistema de throttling** que respete:
  - M√°ximo 3,500 RPM (70% de 5,000)
  - M√°ximo 3,500,000 TPM (70% de 5,000,000)
- **Usar sem√°foros o rate limiters** para controlar la tasa de requests
- **Monitorear en tiempo real** el uso de RPM/TPM

### 5. Manejo de Errores
- **Errores 429 (Rate Limit)**:
  - Implementar backoff exponencial
  - Reintentos autom√°ticos (m√°ximo 5 intentos)
  - Logging detallado del error y tiempo de espera
- **Errores de red**:
  - Reintentos con backoff exponencial
  - Timeout configurable (por defecto: 30 segundos)
- **Errores de procesamiento**:
  - Registrar archivo y chunk que fall√≥
  - Continuar con el siguiente archivo (no detener todo el proceso)
  - Guardar lista de archivos fallidos para reintento posterior

### 6. Logging y Monitoreo
**Logging obligatorio**:
- Archivos procesados (total y exitosos)
- Chunks generados (total)
- Tiempo por archivo (promedio y total)
- Errores (tipo, archivo, chunk)
- Uso de RPM/TPM en tiempo real
- Archivos sospechosos (menos de 5 chunks)

**Formato de logs**:
```python
# Usar logging est√°ndar de Python con niveles apropiados
import logging

logging.info(f"Archivo procesado: {file_name} ({chunks} chunks)")
logging.warning(f"Archivo sospechoso: {file_name} (solo {chunks} chunks)")
logging.error(f"Error procesando {file_name}: {error}")
```

### 7. Archivos Sospechosos
- **Marcar como sospechosos** archivos con menos de 5 chunks
- **Registrar en log** con nivel WARNING
- **Incluir en reporte final** para revisi√≥n manual

### 8. Configuraci√≥n
- **Usar variables de entorno** para par√°metros cr√≠ticos:
  - `MAX_WORKERS`: N√∫mero de workers (default: 15)
  - `EMBEDDING_BATCH_SIZE`: Chunks por request (default: 30)
  - `CHUNK_SIZE`: Tama√±o de chunk (default: 1024) - **NO cambiar sin solicitud**
  - `CHUNK_OVERLAP`: Overlap de chunks (default: 200) - **NO cambiar sin solicitud**
  - `OPENAI_RPM_TARGET`: RPM objetivo (default: 3500)
  - `OPENAI_TPM_TARGET`: TPM objetivo (default: 3500000)
- **Archivo de configuraci√≥n alternativo**: `config_ingesta.py` o similar

### 9. Estructura del Pipeline
El pipeline DEBE seguir este flujo estricto:

```
1. Leer PDF/archivo
   ‚Üì
2. Extraer texto (SimpleDirectoryReader)
   ‚Üì
3. Dividir en chunks (SentenceSplitter con 1024/200)
   ‚Üì
4. Enviar embeddings a OpenAI en lotes (batch size 30-40)
   ‚Üì
5. Guardar embeddings + metadatos en Supabase
```

**NO saltar pasos** ni combinar operaciones de forma que comprometa la claridad.

### 10. Reporte Final
Al terminar la ingesta, generar reporte con:
- N√∫mero de archivos procesados (exitosos y fallidos)
- N√∫mero total de chunks generados
- Tiempo total de procesamiento
- Tiempo promedio por archivo
- Lista de archivos con menos de 5 chunks (sospechosos)
- Lista de archivos fallidos (para reintento)
- Estad√≠sticas de uso de RPM/TPM

## üö´ PROHIBICIONES

1. **NO cambiar chunk size u overlap** sin solicitud expl√≠cita del usuario
2. **NO cambiar el modelo de embeddings** sin solicitud expl√≠cita del usuario
3. **NO exceder el 70% de los l√≠mites de Tier 3** (3,500 RPM, 3,500,000 TPM)
4. **NO procesar sin logging** adecuado
5. **NO ignorar errores** - siempre registrar y manejar
6. **NO usar batch sizes mayores a 40** para embeddings

## ‚úÖ MEJORES PR√ÅCTICAS

1. **C√≥digo modular**: Separar lectura, chunking, embeddings y almacenamiento
2. **Configuraci√≥n centralizada**: Usar variables de entorno o archivo de config
3. **Manejo robusto de errores**: Try-catch en cada etapa cr√≠tica
4. **Logging detallado**: Facilitar debugging y monitoreo
5. **Documentaci√≥n**: Comentar c√≥digo complejo o cr√≠tico
6. **Testing**: Probar con archivos peque√±os antes de procesar lotes grandes

## üìù NOTAS IMPORTANTES

- Estas reglas aplican a **TODO el repositorio** (`**/*`)
- Cualquier cambio a los valores por defecto debe ser **expl√≠citamente solicitado** por el usuario
- El objetivo es mantener un sistema **r√°pido, estable y dentro de los l√≠mites de OpenAI Tier 3**

